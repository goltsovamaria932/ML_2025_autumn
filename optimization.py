# -*- coding: utf-8 -*-
"""optimization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14JfQ_lvaYWOA8bbPeRtTdaNUtbtE2rMm
"""

# Импорт необходимых библиотек
import numpy as np  # Библиотека для численных вычислений
from numpy.linalg import LinAlgError  # Исключения для линейной алгебры
import scipy  # Библиотека для научных вычислений
from scipy.optimize import line_search  # Функция для поиска шага
from datetime import datetime  # Для работы со временем
from collections import defaultdict  # Словарь со значениями по умолчанию


class LineSearchTool(object):
    """
    Инструмент для поиска шага в методах оптимизации.

    method : Строка с методом ('Wolfe', 'Armijo' или 'Constant')
        Метод выбора размера шага.
    kwargs :
        Дополнительные параметры для метода поиска шага.
    """

    def __init__(self, method='Wolfe', **kwargs):
        # Сохраняем выбранный метод
        self._method = method

        # Инициализация параметров в зависимости от метода
        if self._method == 'Wolfe':
            # Параметры для условий Вулфа
            self.c1 = kwargs.get('c1', 1e-4)  # Параметр для условия Армихо
            self.c2 = kwargs.get('c2', 0.9)   # Параметр для условия кривизны
            self.alpha_0 = kwargs.get('alpha_0', 1.0)  # Начальный размер шага
        elif self._method == 'Armijo':
            # Параметры для правила Армихо
            self.c1 = kwargs.get('c1', 1e-4)  # Параметр условия Армихо
            self.alpha_0 = kwargs.get('alpha_0', 1.0)  # Начальный размер шага
        elif self._method == 'Constant':
            # Постоянный размер шага
            self.c = kwargs.get('c', 1.0)  # Постоянное значение шага
        else:
            # Ошибка при неизвестном методе
            raise ValueError('Unknown method {}'.format(method))

    @classmethod
    def from_dict(cls, options):
        """Создает экземпляр класса из словаря с параметрами."""
        # Проверяем, что передан словарь
        if type(options) != dict:
            raise TypeError('LineSearchTool initializer must be of type dict')
        # Создаем экземпляр с распакованными параметрами
        return cls(**options)

    def to_dict(self):
        """Возвращает атрибуты класса в виде словаря."""
        return self.__dict__

    def line_search(self, oracle, x_k, d_k, previous_alpha=None):
        """
        Находит размер шага alpha для точки x_k и направления d_k.

        Parameters:
        oracle : объект оракула
            Оракул с методами вычисления функции и градиента
        x_k : np.array
            Текущая точка
        d_k : np.array
            Направление поиска
        previous_alpha : float или None
            Предыдущий размер шага для инициализации

        Returns:
        alpha : float или None
            Найденный размер шага или None при ошибке
        """

        # Для постоянного шага просто возвращаем константу
        if self._method == 'Constant':
            return self.c

        # Выбираем начальный шаг: предыдущий или по умолчанию
        alpha_0 = self.alpha_0 if previous_alpha is None else previous_alpha

        if self._method == 'Wolfe':
            # Пробуем использовать условия Вулфа из SciPy
            result = line_search(oracle.func, oracle.grad, x_k, d_k,
                               gfk=oracle.grad(x_k),  # Градиент в текущей точке
                               old_fval=oracle.func(x_k),  # Значение функции в текущей точке
                               c1=self.c1, c2=self.c2)  # Параметры условий

            alpha = result[0]  # Извлекаем найденный шаг

            if alpha is not None:
                # Если шаг найден, возвращаем его
                return alpha
            else:
                # Если не найден, используем запасной вариант - правило Армихо
                return self._armijo_backtracking(oracle, x_k, d_k, alpha_0)

        elif self._method == 'Armijo':
            # Используем правило Армихо
            return self._armijo_backtracking(oracle, x_k, d_k, alpha_0)
        else:
            # На всякий случай возвращаем None
            return None

    def _armijo_backtracking(self, oracle, x_k, d_k, alpha_0):
        """
        Поиск шага по правилу Армихо с бэктрэкингом.
        """
        # Вычисляем значение функции и производной в начальной точке (alpha=0)
        phi_0 = oracle.func_directional(x_k, d_k, 0)  # Значение функции
        derphi_0 = oracle.grad_directional(x_k, d_k, 0)  # Производная по направлению

        # Начинаем с начального шага
        alpha = alpha_0

        # Проверяем, что направление является направлением убывания
        if derphi_0 >= 0:
            return None  # Если нет - возвращаем ошибку

        # Максимальное число итераций для защиты от бесконечного цикла
        max_iter = 100

        for i in range(max_iter):
            # Вычисляем значение функции в новой точке
            phi_alpha = oracle.func_directional(x_k, d_k, alpha)

            # Проверяем условие Армихо
            if phi_alpha <= phi_0 + self.c1 * alpha * derphi_0:
                return alpha  # Условие выполнено

            # Уменьшаем шаг вдвое
            alpha /= 2.0

            # Проверяем, не стал ли шаг слишком маленьким
            if alpha < 1e-16:
                return None  # Избегаем численных проблем

        return None  # Не нашли подходящий шаг


def get_line_search_tool(line_search_options=None):
    """Создает инструмент поиска шага из различных форматов входных данных."""
    if line_search_options is None:
        # Если параметры не заданы, создаем инструмент по умолчанию
        return LineSearchTool()
    elif type(line_search_options) is LineSearchTool:
        # Если уже передан объект LineSearchTool, возвращаем как есть
        return line_search_options
    else:
        # Если передан словарь, создаем инструмент из словаря
        return LineSearchTool.from_dict(line_search_options)


def gradient_descent(oracle, x_0, tolerance=1e-5, max_iter=10000,
                     line_search_options=None, trace=False, display=False):
    """
    Метод градиентного спуска для оптимизации.

    Parameters:
    oracle : объект оракула
        Оракул для вычисления функции, градиента и гессиана
    x_0 : np.array
        Начальная точка
    tolerance : float
        Точность для критерия остановки
    max_iter : int
        Максимальное число итераций
    line_search_options : dict, LineSearchTool или None
        Настройки поиска шага
    trace : bool
        Флаг сохранения истории
    display : bool
        Флаг вывода отладочной информации

    Returns:
    x_star : np.array
        Найденная точка
    message : string
        Сообщение о результате
    history : dict или None
        История оптимизации
    """

    # Инициализируем историю, если нужно
    history = defaultdict(list) if trace else None

    # Получаем инструмент поиска шага
    line_search_tool = get_line_search_tool(line_search_options)

    # Копируем начальную точку
    x_k = x_0.copy()

    # Засекаем время начала
    start_time = datetime.now()

    # Вычисляем значение функции и градиента в начальной точке
    f_k = oracle.func(x_k)
    grad_k = oracle.grad(x_k)

    # Проверяем на численные ошибки в начальной точке
    if (np.any(np.isnan(grad_k)) or np.any(np.isinf(grad_k)) or
        np.isnan(f_k) or np.isinf(f_k)):
        return x_k, 'computational_error', history

    # Вычисляем квадрат нормы градиента
    grad_norm_sq = np.linalg.norm(grad_k) ** 2

    # Сохраняем начальную норму градиента для относительного критерия
    grad_norm_0_sq = grad_norm_sq

    # Вычисляем критерий остановки (относительный)
    tolerance_sq = tolerance * grad_norm_0_sq

    # Переменная для хранения предыдущего шага
    previous_alpha = None

    # Сохраняем начальную точку в истории
    if trace:
        current_time = (datetime.now() - start_time).total_seconds()
        history['time'].append(current_time)
        history['func'].append(f_k)
        history['grad_norm'].append(np.sqrt(grad_norm_sq))
        if x_k.size <= 2:  # Сохраняем траекторию только для малой размерности
            history['x'].append(x_k.copy())

    # Проверяем критерий остановки в начальной точке
    if grad_norm_sq <= tolerance_sq:
        return x_k, 'success', history

    # Основной цикл градиентного спуска
    for k in range(max_iter):
        # Вычисляем направление спуска (антиградиент)
        d_k = -grad_k

        # Ищем оптимальный размер шага
        alpha_k = line_search_tool.line_search(oracle, x_k, d_k, previous_alpha)

        # Проверяем валидность найденного шага
        if alpha_k is None or alpha_k <= 0:
            return x_k, 'computational_error', history

        # Обновляем точку: x_{k+1} = x_k + alpha_k * d_k
        x_k = x_k + alpha_k * d_k

        # Сохраняем текущий шаг для следующей итерации
        previous_alpha = alpha_k

        # Вычисляем новые значение функции и градиента
        f_k = oracle.func(x_k)
        grad_k = oracle.grad(x_k)
        grad_norm_sq = np.linalg.norm(grad_k) ** 2

        # Проверяем на численные ошибки
        if (np.any(np.isnan(grad_k)) or np.any(np.isinf(grad_k)) or
            np.isnan(f_k) or np.isinf(f_k)):
            return x_k, 'computational_error', history

        # Сохраняем информацию в истории
        if trace:
            current_time = (datetime.now() - start_time).total_seconds()
            history['time'].append(current_time)
            history['func'].append(f_k)
            history['grad_norm'].append(np.sqrt(grad_norm_sq))
            if x_k.size <= 2:
                history['x'].append(x_k.copy())

        # Выводим отладочную информацию
        if display:
            print(f"Iteration {k}: f(x) = {f_k:.6f}, ||grad|| = {np.sqrt(grad_norm_sq):.6f}, alpha = {alpha_k:.6f}")

        # Проверяем критерий остановки
        if grad_norm_sq <= tolerance_sq:
            return x_k, 'success', history

    # Если вышли по числу итераций, проверяем финальную точку
    grad_final = oracle.grad(x_k)

    # Проверяем на численные ошибки в финальной точке
    if np.any(np.isnan(grad_final)) or np.any(np.isinf(grad_final)):
        return x_k, 'computational_error', history

    # Вычисляем норму градиента в финальной точке
    grad_norm_final_sq = np.linalg.norm(grad_final) ** 2

    # Проверяем критерий остановки в финальной точке
    if grad_norm_final_sq <= tolerance_sq:
        return x_k, 'success', history
    else:
        return x_k, 'iterations_exceeded', history


def newton(oracle, x_0, tolerance=1e-5, max_iter=100,
           line_search_options=None, trace=False, display=False):
    """
    Метод Ньютона для оптимизации.

    Parameters:
    oracle : объект оракула
        Оракул для вычисления функции, градиента и гессиана
    x_0 : np.array
        Начальная точка
    tolerance : float
        Точность для критерия остановки
    max_iter : int
        Максимальное число итераций
    line_search_options : dict, LineSearchTool или None
        Настройки поиска шага
    trace : bool
        Флаг сохранения истории
    display : bool
        Флаг вывода отладочной информации

    Returns:
    x_star : np.array
        Найденная точка
    message : string
        Сообщение о результате
    history : dict или None
        История оптимизации
    """

    # Инициализируем историю
    history = defaultdict(list) if trace else None

    # Получаем инструмент поиска шага
    line_search_tool = get_line_search_tool(line_search_options)

    # Копируем начальную точку
    x_k = x_0.copy()

    # Засекаем время начала
    start_time = datetime.now()

    # Вычисляем значение функции и градиента в начальной точке
    f_k = oracle.func(x_k)
    grad_k = oracle.grad(x_k)

    # Проверяем на численные ошибки в начальной точке
    if (np.any(np.isnan(grad_k)) or np.any(np.isinf(grad_k)) or
        np.isnan(f_k) or np.isinf(f_k)):
        return x_k, 'computational_error', history

    # Вычисляем квадрат нормы градиента
    grad_norm_sq = np.linalg.norm(grad_k) ** 2

    # Сохраняем начальную норму градиента
    grad_norm_0_sq = grad_norm_sq

    # Вычисляем критерий остановки
    tolerance_sq = tolerance * grad_norm_0_sq

    # Сохраняем начальную точку в истории
    if trace:
        current_time = (datetime.now() - start_time).total_seconds()
        history['time'].append(current_time)
        history['func'].append(f_k)
        history['grad_norm'].append(np.sqrt(grad_norm_sq))
        if x_k.size <= 2:
            history['x'].append(x_k.copy())

    # Проверяем критерий остановки в начальной точке
    if grad_norm_sq <= tolerance_sq:
        return x_k, 'success', history

    # Основной цикл метода Ньютона
    for k in range(max_iter):
        # Вычисляем матрицу Гессе (вторые производные)
        hess_k = oracle.hess(x_k)

        # Проверяем гессиан на численные ошибки
        if np.any(np.isnan(hess_k)) or np.any(np.isinf(hess_k)):
            return x_k, 'computational_error', history

        try:
            # Пробуем разложение Холецкого для положительно определенной матрицы
            L = scipy.linalg.cholesky(hess_k, lower=True)
            # Решаем систему: H * d = -g
            d_k = scipy.linalg.cho_solve((L, True), -grad_k)
        except (LinAlgError, ValueError):
            # Если разложение не удалось (матрица не положительно определена)
            return x_k, 'newton_direction_error', history

        # Ищем размер шага, начиная с 1.0 (типично для метода Ньютона)
        alpha_k = line_search_tool.line_search(oracle, x_k, d_k, previous_alpha=1.0)

        # Проверяем валидность шага
        if alpha_k is None or alpha_k <= 0:
            return x_k, 'computational_error', history

        # Обновляем точку
        x_k = x_k + alpha_k * d_k

        # Вычисляем новые значение функции и градиента
        f_k = oracle.func(x_k)
        grad_k = oracle.grad(x_k)
        grad_norm_sq = np.linalg.norm(grad_k) ** 2

        # Проверяем на численные ошибки
        if (np.any(np.isnan(grad_k)) or np.any(np.isinf(grad_k)) or
            np.isnan(f_k) or np.isinf(f_k)):
            return x_k, 'computational_error', history

        # Сохраняем информацию в истории
        if trace:
            current_time = (datetime.now() - start_time).total_seconds()
            history['time'].append(current_time)
            history['func'].append(f_k)
            history['grad_norm'].append(np.sqrt(grad_norm_sq))
            if x_k.size <= 2:
                history['x'].append(x_k.copy())

        # Выводим отладочную информацию
        if display:
            print(f"Iteration {k}: f(x) = {f_k:.6f}, ||grad|| = {np.sqrt(grad_norm_sq):.6f}, alpha = {alpha_k:.6f}")

        # Проверяем критерий остановки
        if grad_norm_sq <= tolerance_sq:
            return x_k, 'success', history

    # Проверяем финальную точку после выхода по числу итераций
    grad_final = oracle.grad(x_k)

    # Проверяем на численные ошибки
    if np.any(np.isnan(grad_final)) or np.any(np.isinf(grad_final)):
        return x_k, 'computational_error', history

    # Вычисляем норму градиента в финальной точке
    grad_norm_final_sq = np.linalg.norm(grad_final) ** 2

    # Проверяем критерий остановки
    if grad_norm_final_sq <= tolerance_sq:
        return x_k, 'success', history
    else:
        return x_k, 'iterations_exceeded', history