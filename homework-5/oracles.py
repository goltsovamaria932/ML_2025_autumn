# -*- coding: utf-8 -*-
"""oracles.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KFFO2cCLjzIOzntU_lPVXzwickUrRQJd
"""

# Импорт необходимых библиотек
import numpy as np  # Основная библиотека для численных вычислений
import scipy  # Библиотека для научных вычислений
from scipy.special import expit  # Функция сигмоиды (логистической функции)
import scipy.sparse  # Работа с разреженными матрицами


class BaseSmoothOracle(object):
    """
    Базовый класс для реализации оракулов (функций с градиентом и гессианом).
    """

    def func(self, x):
        """
        Вычисляет значение функции в точке x.
        """
        # Метод должен быть реализован в дочерних классах
        raise NotImplementedError('Func oracle is not implemented.')

    def grad(self, x):
        """
        Вычисляет градиент функции в точке x.
        """
        # Метод должен быть реализован в дочерних классах
        raise NotImplementedError('Grad oracle is not implemented.')

    def hess(self, x):
        """
        Вычисляет матрицу Гессе (вторых производных) в точке x.
        """
        # Метод должен быть реализован в дочерних классах
        raise NotImplementedError('Hessian oracle is not implemented.')

    def func_directional(self, x, d, alpha):
        """
        Вычисляет phi(alpha) = f(x + alpha*d) - значение функции вдоль направления.

        Parameters:
        x : текущая точка
        d : направление поиска
        alpha : шаг вдоль направления
        """
        # Вычисляем f(x + alpha*d) и убираем лишние измерения
        return np.squeeze(self.func(x + alpha * d))

    def grad_directional(self, x, d, alpha):
        """
        Вычисляет производную phi'(alpha) = (f(x + alpha*d))'_{alpha}

        Parameters:
        x : текущая точка
        d : направление поиска
        alpha : шаг вдоль направления
        """
        # Вычисляем градиент в новой точке и проектируем на направление d
        return np.squeeze(self.grad(x + alpha * d).dot(d))


class QuadraticOracle(BaseSmoothOracle):
    """
    Оракул для квадратичной функции:
       func(x) = 1/2 x^TAx - b^Tx.
    """

    def __init__(self, A, b):
        # Проверяем, что матрица A симметрична
        if scipy.sparse.issparse(A):
            # Для разреженных матриц проверяем приблизительную симметричность данных
            if not np.allclose(A.data, A.T.data):
                raise ValueError('A should be a symmetric matrix.')
        else:
            # Для плотных матриц проверяем точную симметричность
            if not np.allclose(A, A.T):
                raise ValueError('A should be a symmetric matrix.')
        # Сохраняем матрицу A и вектор b
        self.A = A
        self.b = b

    def func(self, x):
        # Вычисляем значение квадратичной функции: 1/2 x^T A x - b^T x
        return 0.5 * np.dot(self.A.dot(x), x) - self.b.dot(x)

    def grad(self, x):
        # Вычисляем градиент: A x - b
        return self.A.dot(x) - self.b

    def hess(self, x):
        # Матрица Гессе для квадратичной функции постоянна и равна A
        return self.A


class LogRegL2Oracle(BaseSmoothOracle):
    """
    Оракул для логистической регрессии с L2-регуляризацией.
    Функция: f(x) = 1/m sum_i log(1 + exp(-b_i * a_i^T x)) + regcoef/2 * ||x||^2
    """

    def __init__(self, matvec_Ax, matvec_ATx, matmat_ATsA, b, regcoef):
        # Сохраняем функции для матричных операций (позволяют работать с разреженными матрицами)
        self.matvec_Ax = matvec_Ax  # Функция для умножения A на x
        self.matvec_ATx = matvec_ATx  # Функция для умножения A^T на x
        self.matmat_ATsA = matmat_ATsA  # Функция для вычисления A^T * diag(s) * A
        self.b = b  # Вектор меток (обычно +1/-1)
        self.regcoef = regcoef  # Коэффициент регуляризации
        self.m = len(b)  # Количество примеров в выборке

    def func(self, x):
        # Вычисляем произведение матрицы на вектор: A x
        Ax = self.matvec_Ax(x)

        # Вычисляем z = -b_i * (A x)_i для каждого примера
        z = -self.b * Ax

        # Вычисляем логистическую потерю: среднее от log(1 + exp(z))
        # logaddexp(0, z) = log(exp(0) + exp(z)) = log(1 + exp(z))
        logistic_loss = np.logaddexp(0, z).mean()

        # Вычисляем регуляризационный член: regcoef/2 * ||x||^2
        reg_term = 0.5 * self.regcoef * np.dot(x, x)

        # Суммируем логистическую потерю и регуляризацию
        return logistic_loss + reg_term

    def grad(self, x):
        # Вычисляем A x
        Ax = self.matvec_Ax(x)

        # Вычисляем z = -b_i * (A x)_i
        z = -self.b * Ax

        # Вычисляем сигмоиду: sigma(z) = 1 / (1 + exp(-z))
        sigma = expit(z)

        # Градиент логистической потери: -1/m * A^T @ (b * sigma)
        grad_logistic = -self.matvec_ATx(self.b * sigma) / self.m

        # Градиент регуляризации: regcoef * x
        grad_reg = self.regcoef * x

        # Суммируем градиенты
        return grad_logistic + grad_reg

    def hess(self, x):
        # Вычисляем A x
        Ax = self.matvec_Ax(x)

        # Вычисляем z = -b_i * (A x)_i
        z = -self.b * Ax

        # Вычисляем сигмоиду
        sigma = expit(z)

        # Вычисляем диагональные элементы s_i = sigma(z_i) * (1 - sigma(z_i)) / m
        # Это вторые производные логистической функции
        s = sigma * (1 - sigma) / self.m

        # Вычисляем матрицу Гессе для логистической части: A^T @ diag(s) @ A
        hess_logistic = self.matmat_ATsA(s)

        # Добавляем матрицу Гессе для регуляризации: regcoef * I
        if scipy.sparse.issparse(hess_logistic):
            # Для разреженных матриц создаем разреженную единичную матрицу
            n = hess_logistic.shape[0]
            hess_reg = self.regcoef * scipy.sparse.identity(n)
        else:
            # Для плотных матриц создаем обычную единичную матрицу
            n = hess_logistic.shape[0]
            hess_reg = self.regcoef * np.eye(n)

        # Суммируем матрицы Гессе
        return hess_logistic + hess_reg


class LogRegL2OptimizedOracle(LogRegL2Oracle):
    """
    Оптимизированный оракул для логистической регрессии с кэшированием вычислений.
    Оптимизирует методы func_directional и grad_directional для поиска шага.
    """

    def __init__(self, matvec_Ax, matvec_ATx, matmat_ATsA, b, regcoef):
        # Вызываем конструктор родительского класса
        super().__init__(matvec_Ax, matvec_ATx, matmat_ATsA, b, regcoef)
        # Инициализируем кэш для оптимизации вычислений
        self._x_cache = None  # Кэш для точки x
        self._Ax_cache = None  # Кэш для A x
        self._Ad_cache = None  # Кэш для A d

    def _update_cache(self, x, d):
        """Обновляет кэш, если точка x изменилась."""
        if (self._x_cache is None or
            # Проверяем, изменилась ли точка x (с учетом численной точности)
            not np.allclose(x, self._x_cache, rtol=1e-10, atol=1e-10)):
            # Обновляем кэш
            self._x_cache = x.copy()  # Сохраняем копию точки x
            self._Ax_cache = self.matvec_Ax(x)  # Вычисляем и кэшируем A x
            self._Ad_cache = self.matvec_Ax(d)  # Вычисляем и кэшируем A d

    def func_directional(self, x, d, alpha):
        # Обновляем кэш для текущих x и d
        self._update_cache(x, d)

        # Вычисляем A(x + alpha*d) = A x + alpha * A d (используя кэш)
        Ax_alpha = self._Ax_cache + alpha * self._Ad_cache

        # Вычисляем z = -b_i * (A(x + alpha*d))_i
        z = -self.b * Ax_alpha

        # Вычисляем логистическую потерю в новой точке
        logistic_loss = np.logaddexp(0, z).mean()

        # Вычисляем новую точку: x + alpha*d
        x_alpha = x + alpha * d

        # Вычисляем регуляризационный член в новой точке
        reg_term = 0.5 * self.regcoef * np.dot(x_alpha, x_alpha)

        return logistic_loss + reg_term

    def grad_directional(self, x, d, alpha):
        # Обновляем кэш для текущих x и d
        self._update_cache(x, d)

        # Вычисляем A(x + alpha*d) = A x + alpha * A d
        Ax_alpha = self._Ax_cache + alpha * self._Ad_cache

        # Вычисляем z = -b_i * (A(x + alpha*d))_i
        z = -self.b * Ax_alpha

        # Вычисляем сигмоиду в новой точке
        sigma = expit(z)

        # Вычисляем производную логистической потери по направлению:
        # -1/m * (A d)^T @ (b * sigma)
        grad_logistic_directional = -np.dot(self._Ad_cache, self.b * sigma) / self.m

        # Вычисляем новую точку
        x_alpha = x + alpha * d

        # Вычисляем производную регуляризации по направлению: regcoef * (x + alpha*d)^T d
        grad_reg_directional = self.regcoef * np.dot(x_alpha, d)

        return grad_logistic_directional + grad_reg_directional


def create_log_reg_oracle(A, b, regcoef, oracle_type='usual'):
    """
    Вспомогательная функция для создания оракулов логистической регрессии.

    Parameters:
    A : матрица признаков (разреженная или плотная)
    b : вектор меток
    regcoef : коэффициент регуляризации
    oracle_type : тип оракула ('usual' или 'optimized')
    """

    # Определяем функции матричных операций в зависимости от типа матрицы A
    if scipy.sparse.issparse(A):
        # Для разреженных матриц используем методы scipy.sparse
        def matvec_Ax(x):
            return A.dot(x)  # Умножение разреженной матрицы на вектор

        def matvec_ATx(x):
            return A.T.dot(x)  # Умножение транспонированной матрицы на вектор

        def matmat_ATsA(s):
            # Вычисление A^T @ diag(s) @ A для разреженных матриц
            return A.T.dot(scipy.sparse.diags(s).dot(A))
    else:
        # Для плотных матриц используем обычное умножение
        def matvec_Ax(x):
            return A @ x  # Матричное умножение

        def matvec_ATx(x):
            return A.T @ x  # Умножение на транспонированную матрицу

        def matmat_ATsA(s):
            # Эффективное вычисление A^T @ diag(s) @ A через broadcasting
            return (A.T * s) @ A

    # Выбираем класс оракула в зависимости от типа
    if oracle_type == 'usual':
        oracle = LogRegL2Oracle
    elif oracle_type == 'optimized':
        oracle = LogRegL2OptimizedOracle
    else:
        raise ValueError('Unknown oracle_type=%s' % oracle_type)

    # Создаем и возвращаем экземпляр оракула
    return oracle(matvec_Ax, matvec_ATx, matmat_ATsA, b, regcoef)


def grad_finite_diff(func, x, eps=1e-8):
    """
    Вычисляет приближение градиента методом конечных разностей.

    Parameters:
    func : функция f(x)
    x : точка, в которой вычисляется градиент
    eps : шаг для конечных разностей

    Returns:
    grad : приближенный градиент
    """
    n = len(x)  # Размерность пространства
    grad = np.zeros(n)  # Инициализируем вектор градиента нулями
    f_x = func(x)  # Вычисляем значение функции в точке x

    # Вычисляем частные производные по каждой координате
    for i in range(n):
        e_i = np.zeros(n)  # Создаем базисный вектор
        e_i[i] = 1.0  # Единица на i-й позиции
        f_x_eps = func(x + eps * e_i)  # Значение функции в точке x + eps*e_i
        # Конечная разность: (f(x+eps) - f(x)) / eps
        grad[i] = (f_x_eps - f_x) / eps

    return grad


def hess_finite_diff(func, x, eps=1e-5):
    """
    Вычисляет приближение матрицы Гессе методом конечных разностей.

    Parameters:
    func : функция f(x)
    x : точка, в которой вычисляется Гессиан
    eps : шаг для конечных разностей

    Returns:
    hess : приближенная матрица Гессе
    """
    n = len(x)  # Размерность пространства
    hess = np.zeros((n, n))  # Инициализируем матрицу Гессе нулями
    f_x = func(x)  # Значение функции в точке x

    # Вычисляем вторые частные производные
    for i in range(n):
        e_i = np.zeros(n)  # Базисный вектор по i-й координате
        e_i[i] = 1.0
        f_x_eps_i = func(x + eps * e_i)  # f(x + eps*e_i)

        for j in range(n):
            e_j = np.zeros(n)  # Базисный вектор по j-й координате
            e_j[j] = 1.0
            f_x_eps_j = func(x + eps * e_j)  # f(x + eps*e_j)
            f_x_eps_ij = func(x + eps * e_i + eps * e_j)  # f(x + eps*e_i + eps*e_j)

            # Формула для смешанной производной:
            # (f(x+eps_i+eps_j) - f(x+eps_i) - f(x+eps_j) + f(x)) / eps^2
            hess[i, j] = (f_x_eps_ij - f_x_eps_i - f_x_eps_j + f_x) / (eps * eps)

    return hess